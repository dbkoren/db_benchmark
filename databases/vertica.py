"""
Docker run vertica:
docker run -p 5433:5433 -p 5444:5444 \
           --mount type=volume,source=vertica-data,target=/data \
           --name vertica_ce \
           vertica/vertica-ce
"""
import vertica_python
from databases.DBBenchmark import DBBenchmark
conn_info = {'host': '127.0.0.1',
             'port': 5433,
             'user': 'DBADMIN',
            #  'password': 'some_password',
             'database': 'VMart',
             # autogenerated session label by default,
             'session_label': 'some_label',
             # default throw error on invalid UTF-8 results
             'unicode_error': 'strict',
             # SSL is disabled by default
             'ssl': False,
             # autocommit is off by default
            #  'autocommit': True,
             # using server-side prepared statements is disabled by default
            #  'use_prepared_statements': False,
             # connection timeout is not enabled by default
             # 5 seconds timeout for a socket operation (Establishing a TCP connection or read/write operation)
             'connection_timeout': 5}

class VerticaBenchmark(DBBenchmark):
    TABLE_NAME = "tbl"
    SCHEMA = "(a INT, b VARCHAR)"

    def __init__(self, table_name=None, schema=None) -> None:
        self.TABLE_NAME = table_name or self.TABLE_NAME
        self.SCHEMA = schema or self.SCHEMA
        self.schema_type_dict = self._split_str_schema_to_type_dict(self.SCHEMA)
        self.connection = vertica_python.connect(**conn_info)
        self.cur = self.connection.cursor()

    def create_table(self, table_name: str):
        try:
            self.cur.execute(f"CREATE TABLE {table_name} ({self.SCHEMA})")
            self.commit_changes()
        except vertica_python.errors.DuplicateObject:
            print(f"Table {table_name} already exists.")

    def insert_row_to_table(self, data):
        self.cur.execute(f"INSERT INTO {self.TABLE_NAME} VALUES ({self._get_str_of_value_list(data)})")
        self.commit_changes()

    def insert_bulk_to_table(self, data_bulk):
        self.cur.executemany(f"INSERT INTO {self.TABLE_NAME} ({','.join(self.schema_type_dict.keys())}) VALUES ({self._get_values_for_insert_bulk_syntax(self.SCHEMA)})",
         self._get_data_bulk_as_list_of_tuples(data_bulk), use_prepared_statements=True)
        self.commit_changes()

    def _get_values_for_insert_bulk_syntax(self, schema):
        return ",".join(['?' for _ in range(schema.count(',') + 1)])
    
    def _get_data_bulk_as_list_of_tuples(self, data_bulk):
        return [self._split_row_to_value_list(row) for row in data_bulk]
    
    def _split_row_to_value_list(self, data_row):
        list_of_row_values = []
        fields = data_row.replace(', ', ',').split(',')
        for field in fields:
            field_name, field_value = field.split(' ', 1)
            # list_of_row_values.append(self.sql_to_python_type(field_name)(field_value))
            list_of_row_values.append(field_value)
        return list_of_row_values

    def query(self, query):
        res = []
        self.cur.execute(query)
        while True:
            rows = self.cur.fetchall()
            res += rows
            if not self.cur.nextset():
                break
        return res
    
    def close_connection(self):
        self.connection.close()
    
    def commit_changes(self):
        self.connection.commit()